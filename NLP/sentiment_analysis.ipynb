{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchtext.datasets import IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  2,   1,   9,  78,   6, 456,   7],\n",
      "        [ 58,   5,  89,   0,   0,   0,   0],\n",
      "        [  2,   0,   0,   0,   0,   0,   0]])\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_sequence\n",
    "\n",
    "seqs = [[58, 5, 89], [2, 1, 9, 78, 6, 456, 7], [2]]\n",
    "\n",
    "seqs = sorted([torch.tensor(x) for x in seqs],\n",
    "              key=lambda elt: elt.size(0), reverse=True,)\n",
    "\n",
    "padded_seqs = pad_sequence(seqs, batch_first=True)\n",
    "print(padded_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 3, 1] torch.Size([3, 7])\n"
     ]
    }
   ],
   "source": [
    "lengths = [elt.size(0) for elt in seqs]\n",
    "print(lengths, padded_seqs.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PackedSequence(data=tensor([  2,  58,   2,   1,   5,   9,  89,  78,   6, 456,   7]), batch_sizes=tensor([3, 2, 2, 1, 1, 1, 1]), sorted_indices=tensor([0, 1, 2]), unsorted_indices=tensor([0, 1, 2]))\n"
     ]
    }
   ],
   "source": [
    "packed_seqs = pack_padded_sequence(\n",
    "    padded_seqs,\n",
    "    lengths=lengths,\n",
    "    batch_first=True,\n",
    ")\n",
    "print(packed_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../data/\"\n",
    "\n",
    "train_data = IMDB(root=data_dir, split=\"train\")\n",
    "test_set = IMDB(root=data_dir, split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = list(train_data)\n",
    "test_set = list(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "train_set, valid_set = random_split(train_data, [20_000, 5_000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def tokenizer(text: str):\n",
    "    text = re.sub(\"<[^>]*>\", \"\", text)\n",
    "    emoticons = re.findall(\"(?::|;|=)(?:-)?(?:\\)|\\(|D|P)\", text.lower())\n",
    "    text = re.sub(\"[\\W]+\", \" \", text.lower()) + \\\n",
    "        \" \".join(emoticons).replace(\"-\", \"\")\n",
    "    tokenized = text.split()\n",
    "\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size is 69241\n"
     ]
    }
   ],
   "source": [
    "import torchtext\n",
    "from collections import Counter, OrderedDict\n",
    "\n",
    "token_counter = Counter()\n",
    "for _, line in train_set:\n",
    "    token_counter.update(tokenizer(line))\n",
    "\n",
    "print(f\"Vocab size is {len(token_counter)}\")\n",
    "\n",
    "# Building the vocabulary\n",
    "sorted_by_freq_tuple = sorted(\n",
    "    token_counter.items(),\n",
    "    key=lambda x: x[1],\n",
    "    reverse=True,\n",
    ")\n",
    "ordered_tokens = OrderedDict(sorted_by_freq_tuple)\n",
    "vocabulary = torchtext.vocab.vocab(ordered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding special tokens indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary.insert_token(\"<pad>\", 0)\n",
    "vocabulary.insert_token(\"<unk>\", 1)\n",
    "vocabulary.set_default_index(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices for  are: 'I'm the leader of project Gutenberg'\n",
      "[10, 143, 2, 2178, 5, 1156, 1]\n"
     ]
    }
   ],
   "source": [
    "eval_sentence = \"I'm the leader of project Gutenberg\"\n",
    "print(f\"Indices for  are: '{eval_sentence}'\")\n",
    "print([vocabulary[token] for token in tokenizer(eval_sentence)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`plt.quiver` prend en entrée quatre matrices, les deux premières représentent les origines des vecteurs et les deux dernières les points terminaux.\n",
    "Ainsi pour:\n",
    "```py\n",
    "# Create some example data for vectors\n",
    "x = [0, 1, 2, 3]\n",
    "y = [0, 1, 0, 1]\n",
    "u = [1, 2, -1, -2]  # x-components of vectors\n",
    "v = [1, -1, 2, -2]  # y-components of vectors\n",
    "\n",
    "# Create a figure and axis\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Draw arrow vectors\n",
    "ax.quiver(x, y, u, v, angles='xy', scale_units='xy',\n",
    "          scale=1, color='blue', label='Vectors')\n",
    "\n",
    "# Set axis limits\n",
    "ax.set_xlim(-1, 4)\n",
    "ax.set_ylim(-2, 3)\n",
    "\n",
    "# Add labels and legend\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.grid()\n",
    "plt.quiver()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "Le premier vecteur est composé des points X1 = $(x[0], y[0])$ et X2 = $(u[0], v[0])$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal, Tuple\n",
    "\n",
    "\n",
    "def to_seq(text: str):\n",
    "    r\"\"\"\n",
    "    Encodes a text into its vector representation using a vocabulary.\n",
    "    \"\"\"\n",
    "    return [vocabulary[token] for token in tokenizer(text)]\n",
    "\n",
    "\n",
    "def label_pipeline(x: Literal[\"pos\", \"neg\"]):\n",
    "    r\"\"\"\n",
    "    Returns the numerical representation of an input sentence's label.\n",
    "    If the label is 'pos' then returns 1, otherwise 0.\n",
    "    \"\"\"\n",
    "    return 1.0 if x == \"pos\" else 0.0\n",
    "\n",
    "\n",
    "def collate_batch(batch: list[Tuple[str, str]]):\n",
    "    label_list, text_list, lengths = [], [], []\n",
    "\n",
    "    for _label, _sentence in batch:\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        processed_text = torch.tensor(to_seq(_sentence), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "        lengths.append(processed_text.size(0))\n",
    "\n",
    "    label_list = torch.tensor(label_list)\n",
    "    lengths = torch.tensor(lengths)\n",
    "    padded_text_list = nn.utils.rnn.pad_sequence(\n",
    "        text_list,\n",
    "        batch_first=True,\n",
    "    )\n",
    "\n",
    "    return padded_text_list, label_list, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "train_dl = DataLoader(\n",
    "    train_set,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_batch,\n",
    ")\n",
    "\n",
    "valid_dl = DataLoader(\n",
    "    valid_set,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_batch,\n",
    ")\n",
    "\n",
    "test_dl = DataLoader(\n",
    "    test_set,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_batch,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomRnn(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        hidden_size: int,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size,\n",
    "            hidden_size,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        _, h = self.rnn(x)\n",
    "        out = h[-1, :, :]\n",
    "\n",
    "        return self.fc(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomRnn(\n",
      "  (rnn): RNN(64, 32, num_layers=2, batch_first=True)\n",
      "  (fc): Linear(in_features=32, out_features=1, bias=True)\n",
      ") tensor([[ 0.1398],\n",
      "        [-0.4366],\n",
      "        [ 0.1169],\n",
      "        [ 0.2668],\n",
      "        [-0.1811]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model = CustomRnn(input_size=64, hidden_size=32).to(device)\n",
    "print(model, model(torch.randn(5, 3, 64)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentRnn(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        embed_dim: int,\n",
    "        rnn_hidden_size: int,\n",
    "        fc_hidden_size: int,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(\n",
    "            vocab_size,\n",
    "            embed_dim,\n",
    "            padding_idx=0,\n",
    "        )\n",
    "        self.rnn = nn.LSTM(\n",
    "            embed_dim,\n",
    "            rnn_hidden_size,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(rnn_hidden_size, fc_hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(fc_hidden_size, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, text: torch.Tensor, lengths: torch.Tensor):\n",
    "        embedded = self.embedding(text)\n",
    "        packed_out = nn.utils.rnn.pack_padded_sequence(\n",
    "            embedded,\n",
    "            lengths.cpu().numpy(),\n",
    "            enforce_sorted=False,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "        _, (h, cell) = self.rnn(packed_out)\n",
    "        out = h[-1, :, :]\n",
    "        return self.fc(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def train(\n",
    "    net: SentimentRnn,\n",
    "    loader: DataLoader,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    loss_fn: nn.Module,\n",
    "):\n",
    "    net.train()\n",
    "    total_acc, total_loss = 0.0, 0.0\n",
    "    loader_items_count = len(loader.dataset)\n",
    "\n",
    "    for texts, labels, lengths in tqdm(loader):\n",
    "        optimizer.zero_grad()\n",
    "        texts, labels, lengths = texts.to(device), labels.to(device), lengths.to(device)\n",
    "        \n",
    "        preds = model(texts, lengths)[:, 0]\n",
    "        loss: torch.Tensor = loss_fn(preds, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        current_acc = (\n",
    "            (\n",
    "                # preds >= .5 retourne des booléens, .float() transforme les bool en 1. et 0.\n",
    "                (preds >= 0.5).float()\n",
    "                == labels\n",
    "            )\n",
    "            .float()\n",
    "            .sum()\n",
    "            .item()\n",
    "        )\n",
    "        total_acc += current_acc\n",
    "\n",
    "        current_loss = loss.item() * labels.size(0)\n",
    "        total_loss += current_loss\n",
    "\n",
    "        print(\n",
    "            f\"Accuracy: {current_acc / len(labels)}, Loss: {current_loss/ len(labels)}\"\n",
    "        )\n",
    "\n",
    "    return total_acc / loader_items_count, total_loss / loader_items_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(\n",
    "    net: SentimentRnn,\n",
    "    loader: DataLoader,\n",
    "    loss_fn: nn.Module,\n",
    "):\n",
    "    net.eval()\n",
    "    total_acc, total_loss = 0.0, 0.0\n",
    "    loader_items_count = len(loader.dataset)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for texts, labels, lengths in tqdm(loader):\n",
    "            texts, labels, lengths = texts.to(device), labels.to(device), lengths.to(device)\n",
    "            preds = model(texts, lengths)[:, 0]\n",
    "            loss: torch.Tensor = loss_fn(preds, labels)\n",
    "            loss.backward()\n",
    "\n",
    "            current_acc = ((preds >= 0.5).float() == labels).float().sum().item()\n",
    "            total_acc += current_acc\n",
    "\n",
    "            current_loss = loss.item() * labels.size(0)\n",
    "            total_loss += current_loss\n",
    "\n",
    "            print(\n",
    "                f\"Accuracy: {current_acc / len(labels)}, Loss: {current_loss/ len(labels)}\"\n",
    "            )\n",
    "\n",
    "    return total_acc / loader_items_count, total_loss / loader_items_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "\n",
    "vocabulary_size = len(vocabulary)\n",
    "embedding_dim = 20\n",
    "rnn_hidden_size = 64\n",
    "fc_hidden_size = 64\n",
    "\n",
    "model = SentimentRnn(\n",
    "    vocab_size=vocabulary_size,\n",
    "    embed_dim=embedding_dim,\n",
    "    rnn_hidden_size=rnn_hidden_size,\n",
    "    fc_hidden_size=fc_hidden_size,\n",
    ")\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=1e-3,\n",
    ")\n",
    "num_epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    acc_train, loss_train = train(\n",
    "        net=model, loader=train_dl, optimizer=optimizer, loss_fn=criterion\n",
    "    )\n",
    "    acc_valid, loss_valid = evaluate(net=model, loader=valid_dl, loss_fn=criterion)\n",
    "    print(\n",
    "        f\"Epoch {epoch} accuracy: {acc_train:.4f}\",\n",
    "        f\" val_accuracy: {acc_valid:.4f}\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atom",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
